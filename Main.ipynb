{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "#os.environ[\"TORCH_USE_CUDA_DSA\"]= str(0)\n",
    "warnings.filterwarnings('ignore') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mne\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import import_ipynb\n",
    "from Model2 import net\n",
    "from Dataloader2 import EEGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNE_Data = EEGDataset(root_dir=\"D:\\TEST MNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dataloader = DataLoader(MNE_Data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net(T = 3000, C = 19, input_size = 60, hidden_size = 30, num_layers=4, spatial_num= 10, dropout=0.2, pool=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in trange(epochs):\\n        model.train()\\n        running_loss = 0.0\\n        correct_num = 0\\n\\n        for index, data in enumerate(train_dataloader):\\n            x, y = data\\n            x = x.reshape(-1, 1, 19, 3000).float()\\n            x, y = x.to(device), y.to(device)\\n            \\n            '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_num = 0\n",
    "\n",
    "        for index, data in enumerate(train_dataloader):\n",
    "            x, y = data\n",
    "            x = x.reshape(-1, 1, 19, 3000).float()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            \"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_num = 0\n",
    "\n",
    "        for index, data in enumerate(train_dataloader):\n",
    "            x, y = data\n",
    "            y = y.to(torch.float64)\n",
    "            x = x.reshape(-1, 1, 19, 3000).float()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            __ = (torch.round(y_pred).to(torch.int64) == y)\n",
    "            preds = np.all(__.cpu().numpy(), axis=1)\n",
    "            correct_num += np.count_nonzero(preds)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        #print(f\"Train Accuracy:{correct_num / train_count}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5780, 0.4220],\n",
       "        [0.5945, 0.4055],\n",
       "        [0.5772, 0.4228],\n",
       "        [0.5616, 0.4384],\n",
       "        [0.5952, 0.4048],\n",
       "        [0.5436, 0.4564],\n",
       "        [0.5942, 0.4058],\n",
       "        [0.5689, 0.4311],\n",
       "        [0.5858, 0.4142],\n",
       "        [0.5884, 0.4116],\n",
       "        [0.5652, 0.4348],\n",
       "        [0.5650, 0.4350],\n",
       "        [0.5671, 0.4329],\n",
       "        [0.5575, 0.4425],\n",
       "        [0.5416, 0.4584],\n",
       "        [0.5847, 0.4153],\n",
       "        [0.5746, 0.4254],\n",
       "        [0.5600, 0.4400],\n",
       "        [0.5580, 0.4420],\n",
       "        [0.5758, 0.4242],\n",
       "        [0.5894, 0.4106],\n",
       "        [0.5421, 0.4579],\n",
       "        [0.5521, 0.4479],\n",
       "        [0.5542, 0.4458],\n",
       "        [0.5728, 0.4272],\n",
       "        [0.6014, 0.3986],\n",
       "        [0.5654, 0.4346],\n",
       "        [0.5667, 0.4333],\n",
       "        [0.5599, 0.4401],\n",
       "        [0.5795, 0.4205],\n",
       "        [0.5763, 0.4237],\n",
       "        [0.5884, 0.4116],\n",
       "        [0.5817, 0.4183],\n",
       "        [0.5895, 0.4105],\n",
       "        [0.5565, 0.4435],\n",
       "        [0.5506, 0.4494],\n",
       "        [0.5775, 0.4225],\n",
       "        [0.5673, 0.4327],\n",
       "        [0.5728, 0.4272],\n",
       "        [0.5843, 0.4157],\n",
       "        [0.5900, 0.4100],\n",
       "        [0.6096, 0.3904],\n",
       "        [0.5921, 0.4079],\n",
       "        [0.5809, 0.4191],\n",
       "        [0.5850, 0.4150],\n",
       "        [0.5528, 0.4472],\n",
       "        [0.5754, 0.4246],\n",
       "        [0.5430, 0.4570],\n",
       "        [0.5745, 0.4255],\n",
       "        [0.5864, 0.4136],\n",
       "        [0.5664, 0.4336],\n",
       "        [0.5633, 0.4367],\n",
       "        [0.5445, 0.4555],\n",
       "        [0.5687, 0.4313],\n",
       "        [0.5988, 0.4012],\n",
       "        [0.5774, 0.4226],\n",
       "        [0.5991, 0.4009],\n",
       "        [0.5788, 0.4212],\n",
       "        [0.5866, 0.4134],\n",
       "        [0.5676, 0.4324],\n",
       "        [0.5537, 0.4463],\n",
       "        [0.5686, 0.4314],\n",
       "        [0.5606, 0.4394],\n",
       "        [0.5845, 0.4155],\n",
       "        [0.5617, 0.4383],\n",
       "        [0.5659, 0.4341],\n",
       "        [0.5688, 0.4312],\n",
       "        [0.5926, 0.4074],\n",
       "        [0.5669, 0.4331],\n",
       "        [0.5675, 0.4325],\n",
       "        [0.6043, 0.3957],\n",
       "        [0.5744, 0.4256],\n",
       "        [0.5428, 0.4572],\n",
       "        [0.5653, 0.4347],\n",
       "        [0.5927, 0.4073],\n",
       "        [0.5983, 0.4017],\n",
       "        [0.5578, 0.4422],\n",
       "        [0.5607, 0.4393],\n",
       "        [0.6033, 0.3967],\n",
       "        [0.5847, 0.4153],\n",
       "        [0.5778, 0.4222],\n",
       "        [0.5394, 0.4606],\n",
       "        [0.5478, 0.4522],\n",
       "        [0.5389, 0.4611],\n",
       "        [0.5559, 0.4441],\n",
       "        [0.5605, 0.4395],\n",
       "        [0.5624, 0.4376],\n",
       "        [0.5758, 0.4242],\n",
       "        [0.5676, 0.4324],\n",
       "        [0.5726, 0.4274],\n",
       "        [0.5521, 0.4479],\n",
       "        [0.5662, 0.4338],\n",
       "        [0.5467, 0.4533],\n",
       "        [0.5665, 0.4335],\n",
       "        [0.6005, 0.3995],\n",
       "        [0.5608, 0.4392],\n",
       "        [0.5827, 0.4173],\n",
       "        [0.5880, 0.4120],\n",
       "        [0.5768, 0.4232],\n",
       "        [0.5778, 0.4222],\n",
       "        [0.5726, 0.4274],\n",
       "        [0.5668, 0.4332],\n",
       "        [0.5694, 0.4306],\n",
       "        [0.5357, 0.4643],\n",
       "        [0.5689, 0.4311],\n",
       "        [0.5703, 0.4297],\n",
       "        [0.5859, 0.4141],\n",
       "        [0.5823, 0.4177],\n",
       "        [0.5514, 0.4486],\n",
       "        [0.5838, 0.4162],\n",
       "        [0.5750, 0.4250],\n",
       "        [0.5759, 0.4241],\n",
       "        [0.5662, 0.4338],\n",
       "        [0.5665, 0.4335],\n",
       "        [0.5375, 0.4625],\n",
       "        [0.5988, 0.4012],\n",
       "        [0.5985, 0.4015],\n",
       "        [0.5580, 0.4420],\n",
       "        [0.5920, 0.4080],\n",
       "        [0.5676, 0.4324],\n",
       "        [0.5838, 0.4162],\n",
       "        [0.5836, 0.4164],\n",
       "        [0.5540, 0.4460],\n",
       "        [0.5506, 0.4494],\n",
       "        [0.5743, 0.4257],\n",
       "        [0.5976, 0.4024],\n",
       "        [0.5567, 0.4433],\n",
       "        [0.5704, 0.4296],\n",
       "        [0.5370, 0.4630],\n",
       "        [0.5775, 0.4225],\n",
       "        [0.5692, 0.4308],\n",
       "        [0.5927, 0.4073],\n",
       "        [0.5840, 0.4160],\n",
       "        [0.5804, 0.4196],\n",
       "        [0.5628, 0.4372],\n",
       "        [0.5634, 0.4366],\n",
       "        [0.5686, 0.4314],\n",
       "        [0.5482, 0.4518],\n",
       "        [0.5663, 0.4337],\n",
       "        [0.5515, 0.4485],\n",
       "        [0.5689, 0.4311],\n",
       "        [0.5675, 0.4325],\n",
       "        [0.5582, 0.4418],\n",
       "        [0.5708, 0.4292],\n",
       "        [0.5654, 0.4346],\n",
       "        [0.5688, 0.4312],\n",
       "        [0.5794, 0.4206],\n",
       "        [0.5671, 0.4329],\n",
       "        [0.5690, 0.4310],\n",
       "        [0.5599, 0.4401],\n",
       "        [0.5714, 0.4286],\n",
       "        [0.5713, 0.4287],\n",
       "        [0.5846, 0.4154],\n",
       "        [0.5763, 0.4237],\n",
       "        [0.5556, 0.4444],\n",
       "        [0.5918, 0.4082],\n",
       "        [0.5796, 0.4204],\n",
       "        [0.5664, 0.4336],\n",
       "        [0.5693, 0.4307],\n",
       "        [0.5468, 0.4532],\n",
       "        [0.5545, 0.4455],\n",
       "        [0.5497, 0.4503],\n",
       "        [0.5805, 0.4195],\n",
       "        [0.5732, 0.4268],\n",
       "        [0.5522, 0.4478],\n",
       "        [0.5764, 0.4236],\n",
       "        [0.5664, 0.4336],\n",
       "        [0.5529, 0.4471],\n",
       "        [0.5600, 0.4400],\n",
       "        [0.5617, 0.4383],\n",
       "        [0.5781, 0.4219],\n",
       "        [0.5485, 0.4515],\n",
       "        [0.5615, 0.4385],\n",
       "        [0.5670, 0.4330],\n",
       "        [0.5620, 0.4380],\n",
       "        [0.5618, 0.4382],\n",
       "        [0.5685, 0.4315],\n",
       "        [0.5740, 0.4260],\n",
       "        [0.5846, 0.4154],\n",
       "        [0.5839, 0.4161],\n",
       "        [0.5854, 0.4146],\n",
       "        [0.5785, 0.4215],\n",
       "        [0.5692, 0.4308],\n",
       "        [0.5763, 0.4237],\n",
       "        [0.5895, 0.4105],\n",
       "        [0.5880, 0.4120],\n",
       "        [0.5458, 0.4542],\n",
       "        [0.5541, 0.4459],\n",
       "        [0.5460, 0.4540],\n",
       "        [0.5808, 0.4192],\n",
       "        [0.5603, 0.4397],\n",
       "        [0.5649, 0.4351],\n",
       "        [0.5475, 0.4525],\n",
       "        [0.5669, 0.4331],\n",
       "        [0.5938, 0.4062],\n",
       "        [0.5921, 0.4079],\n",
       "        [0.5682, 0.4318],\n",
       "        [0.5839, 0.4161],\n",
       "        [0.5648, 0.4352],\n",
       "        [0.5530, 0.4470],\n",
       "        [0.5849, 0.4151],\n",
       "        [0.5528, 0.4472],\n",
       "        [0.5755, 0.4245],\n",
       "        [0.5473, 0.4527],\n",
       "        [0.5843, 0.4157],\n",
       "        [0.5905, 0.4095],\n",
       "        [0.5383, 0.4617],\n",
       "        [0.5584, 0.4416],\n",
       "        [0.5570, 0.4430],\n",
       "        [0.5645, 0.4355],\n",
       "        [0.5918, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
