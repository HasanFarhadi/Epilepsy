{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "#os.environ[\"TORCH_USE_CUDA_DSA\"]= str(0)\n",
    "warnings.filterwarnings('ignore') \n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mne\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.transforms = transforms.ToTensor()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index, ...]\n",
    "        y = self.y[index, ...]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalzie(data):\n",
    "    normals = []\n",
    "    scaler = StandardScaler()\n",
    "    for idx in range(len(data)):\n",
    "        normals.append(scaler.fit_transform(data[idx]))\n",
    "    return np.array(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading D:\\MNE Data\\PNES\\RostamiAlireza.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    9996.67 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "693 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 693 events and 3000 original time points ...\n",
      "Reading D:\\MNE Data\\TLE\\AlipoorMohamadHakim.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    9996.67 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "650 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 650 events and 3000 original time points ...\n"
     ]
    }
   ],
   "source": [
    "PNES_data = mne.read_epochs(r\"D:\\MNE Data\\PNES\\RostamiAlireza.fif\", preload=False).get_data(picks='eeg');\n",
    "TLE_data = mne.read_epochs(r\"D:\\MNE Data\\TLE\\AlipoorMohamadHakim.fif\", preload=False).get_data(picks='eeg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNES_data = Normalzie(PNES_data)\n",
    "TLE_data = Normalzie(TLE_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNES_labels = np.zeros((PNES_data.shape[0]))\n",
    "TLE_labels = np.ones((TLE_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((PNES_data, TLE_data), axis=0).reshape(-1, 1, 19, 3000)\n",
    "labels = np.concatenate((PNES_labels, TLE_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self, batch_size, T, C, input_size, hidden_size, num_layers):\n",
    "        super(network, self).__init__()\n",
    "        self.N = batch_size\n",
    "        self.T = T\n",
    "        self.C = C\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.cell_count = self.T // self.input_size\n",
    "\n",
    "\n",
    "        self._lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "\n",
    "        self.lstm = nn.ModuleList([self._lstm for i in range(self.C)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.N = x.shape[0]\n",
    "        x = x.reshape(self.N, self.C, self.cell_count, self.input_size)\n",
    "        _x = None\n",
    "\n",
    "        for index, cell in enumerate(self.lstm):\n",
    "            cell_out, _ = cell(x[:, index, :, :], None)\n",
    "            last_layer_out = cell_out[:, -1, :]\n",
    "            \n",
    "            last_layer_out.unsqueeze(0)\n",
    "            if _x is None:\n",
    "                _x = last_layer_out\n",
    "            else:\n",
    "                _x = torch.cat((_x, last_layer_out), dim=0)\n",
    "            \n",
    "        _x = _x.reshape(self.C, -1, self.hidden_size)\n",
    "        x = _x.permute(1, 0, 2).unsqueeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([479, 1, 19, 30])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor(479, 3000, 19)\n",
    "a = torch.rand(479, 1, 19, 3000)\n",
    "\n",
    "m = network(batch_size=479, T = 3000, C = 19, input_size = 60, hidden_size = 30, num_layers=4)\n",
    "m(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
